{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hMJTkkjejrW7",
    "outputId": "07588b49-16e9-40af-dac5-1be02fcf90a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 26 12:16:44 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   40C    P8    12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#This is used to see the GPU configuration\n",
    "##In our case we are using googe colab freebie version that is Tesla T4\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wvB37HELj4gy"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense,Embedding,Flatten, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N2K0TpcqkkwE",
    "outputId": "ce9d4bea-793c-494d-e235-742c8694c23a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train,y_train),(X_test,y_test) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YnInQAJhklZz",
    "outputId": "e413e703-a707-4fb3-9d5c-8ffc29fd8a60"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6D-G8vCkklXn"
   },
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train,padding='post',maxlen=50)\n",
    "X_test = pad_sequences(X_test,padding='post',maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xHCQ0ZeRklU-",
    "outputId": "7d8eb3d1-5c06-407b-b5a9-3123182c7354"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 50)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hZ0QQkhMk9cF",
    "outputId": "8193d336-8c29-41af-db8b-ff3484672de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 1)             10000     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 32)                4352      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14385 (56.19 KB)\n",
      "Trainable params: 14385 (56.19 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "##Adding embedding layer to convert into vector representation\n",
    "##Here 10000 represents intructs the model to consider top 10000 unique words and\n",
    "##convert them into their vector respresentation. Please note in train we have 25000 words out of which\n",
    "##We are going to utilize the top 10000 frequent words and there occurrence we are going to observe in the each\n",
    "##Sentence.\n",
    "##output_dim represents the dimensions of output vector resprsentation in space. 1 represents representing in 1\n",
    "##2 represents 2 D .. so on and input_length  is equivalent to max lenght that we have considered above for\n",
    "##while applying padding.\n",
    "##Also, please note that size of Embedding layer is a hyperparameter and may varry as per experimentation\n",
    "model.add(Embedding(10000,output_dim=1,input_length=50))\n",
    "\n",
    "##Adding LSTM layers where 32 represents the number of hidden neuron\n",
    "##See we are variatins of LSTM like one to many, many to many etc. Here \n",
    "##we are solving the sentiment analysis use case where we do not require any sequence in return.\n",
    "##This is the reason why we are setting return_sequence to False. Ideally LSTM takes a sequence and returns the sequece\n",
    "##Since these all are sequece to sequence models. When we just need output and not the return sequence then set\n",
    "##return_sequences=False\n",
    "model.add(LSTM(32,return_sequences=False))\n",
    "\n",
    "##Output layer where we are having only 1 neuron since this is a Classification problem statement\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "odgxVDOgmcXE",
    "outputId": "15ef7d03-2872-45cd-8e9c-9d70e35e68b8"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train,epochs=1,validation_data=(X_test,y_test))\n",
    "\n",
    "##In order to save the time we are only training the model for 1 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important observation:\n",
    "When previous lecture we trained the imdb dataset on top of RNN model which was trained over >1 epocs and was still giving around 50% accuracy score. \n",
    "\n",
    "But, here in case of LSTM when trained over same dataset with just 1 epoch we are getting accuracy of around 80%.\n",
    "\n",
    "This show how good LSTM is over RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dxAQ91GsnAOy",
    "outputId": "b75131f1-ee58-4378-f617-edd37065311f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Now we will be doing the prediction and for that we cannot straight away pass the test datapoints\n",
    "##to the model since dimension of datapoint and the dimension in which model understands the datapoints are\n",
    "##different\n",
    "\n",
    "##Dimension of datapoint before reshape\n",
    "\n",
    "X_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K4MORg62mcU1"
   },
   "outputs": [],
   "source": [
    "##Dimension of the datapoint after reshape\n",
    "\n",
    "test_data = X_test[0][0:50].reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CHPtepnTnEQM",
    "outputId": "deea4cf4-3701-49bc-e12e-e1e30b23671f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please note that our purpose of reshaping here is to covert the input in terms of columns(1,50) which intially is present in terms of rows(50,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UU9vEk1unFoz",
    "outputId": "f49110b7-fd3e-40af-88b3-306aa0bcd5bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 573ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.24039903]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_data)\n",
    "\n",
    "##Here we are getting output as 0.24039903 which is less than 0.5\n",
    "##hence it will be squashed to 0 which is negative sentiment (recall sigmoid actication fun^ basics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNyeIli0nKav"
   },
   "source": [
    "# Logic to squash predicted output to 0 or 1:\n",
    "Here we can write a logic where if predicted_output >=0.5 then it will be treated as 1(positive sentiment) else it will be treated as 0(negative sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please note that we are using inbult dataset called imdb dataset which is already preprocessed. Already preprocessed does not means that it is converted into vector representation. We still need to explicit to do it. Here pre processed dataset means removing noice in terms of stemming, lowercase conversion, lemmitization, html tag removal, special symbol removal etc. Refer to Lec-7 folder where we have a .ipynb implementation of the same.\n",
    "\n",
    "# In real word scenario we will be getting the dataset which was not already preprocessed and will require to perform preprocessing on top of that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigment\n",
    "Dataset: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "\n",
    "So this is unprocessed imdb dataset where you will find html tags, special symbols etc in the **Review** column. \n",
    "We need to preprocess this first. Then if you observe the output label **Sentiment** then it is categorical encoded i.e; positive and negative. We need to perform integer encoding on this where positive will be represented by 1 and negative by 0.\n",
    "\n",
    "Once this is done we may use the name LSTM implementation being discussed here for builting and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
