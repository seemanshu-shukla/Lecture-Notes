{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d31834ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the dataset\n",
    "import pandas as pd\n",
    "data=pd.read_csv(\"https://raw.githubusercontent.com/sunnysavita10/Naive-Bayes/main/SpamClassifier-with-ML/sms_spam_data/SMSSpamCollection.csv\",sep=\"\\t\",header=None,names=[\"labels\",\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52dcfa0",
   "metadata": {},
   "source": [
    "# In above cell why we read the data with sep= \"\\t\" ?\n",
    "Ans: The dataset is avaiable in the tab delimited or separated manner this is the reason why we use \"\\t\" representing escape sequence for tab. Using header = None we are making sure that column header(in case exists) is removed and using names we are giving the name to column headers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30533d5d",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "\n",
    " - text classification\n",
    " - text generation\n",
    " - text summarization\n",
    " - quesiton answer\n",
    " - spot keyword\n",
    " - fill blanks\n",
    " - topic modeling\n",
    " - text similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "27e1ed90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  labels                                           messages\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying first 5 records of dataset in form of dataframe\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f587bf73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First message\n",
    "data[\"messages\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f396627b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ok lar... Joking wif u oni...'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Second message\n",
    "data[\"messages\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42ffa2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Third message\n",
    "data[\"messages\"][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecd79df",
   "metadata": {},
   "source": [
    "# Above we can clearly observe that the message consists of a lot of noise for example multiple dots ... This will be handled later using regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5099468a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sunny\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk  #this is nltk library which can be used to implement most of the NLP concepts like stopwords, stemming,\n",
    "             #lemmitization etc. nltk stands for natural language processing tool kit.\n",
    "\n",
    "import re    #re is the regular expression library used for pattern matching\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer #PorterStemmer is the stemming technique that is used\n",
    "                                           #as a suffix stripping algorithm.\n",
    "nltk.download(\"stopwords\")\n",
    "#In case any module of nltk is not present then use nltk.download(\"stopwords\") where we are mentioning the module\n",
    "#name to be downloaded under quotes\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "368348dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stopwords.words('english') can be used to list down all the stopwords available in the english language. Please note that here\n",
    "#we are not listing stopwords in \"english\" word instead listing all the words within English language as a whole.\n",
    "\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88192e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"messages\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161a9a5e",
   "metadata": {},
   "source": [
    "#### Regular expressions are a sequence of characters that define a search pattern. They can be used to search, match, and manipulate text strings based on specific patterns. The re module allows you to perform various operations using regular expressions, such as pattern matching, substitution, and splitting strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f11b002a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go until jurong point  crazy   Available only in bugis n great world la e buffet    Cine there got amore wat   '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('[^a-zA-Z]',' ',data[\"messages\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83a185d",
   "metadata": {},
   "source": [
    "# Here, we are using substitite method of regular expression lib where:\n",
    " - In 1st parameter '[^a-zA-Z]' = we are defining the pattern that we want to keep as part of output which is all the sequences STARTING WITH lower or upper case english alphabets.\n",
    "\n",
    " - In 2nd parameter ' ' = we are mentioning the string with which all the unmatached string(as defined in the 1st parameter) will get substituted.\n",
    " \n",
    " - In 3rd parameter data[\"messages\"][0] = we are passing the data on which substituion using regular expression needs to be implemented\n",
    " \n",
    "In the above example we can clearly observe that before applying substitution using regular expression 0th message was containing a lot of noise such as '.' and ','. And after applying substitution all the respective noises were substituted with ' ' ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba2d05cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go until jurong point  crazy   available only in bugis n great world la e buffet    cine there got amore wat   '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Followed by re.sub() lowering the words\n",
    "\n",
    "re.sub('[^a-zA-Z]',' ',data[\"messages\"][0]).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0095c5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using split() to split into words using sep=' ' and store it in the list.\n",
    "\n",
    "words=re.sub('[^a-zA-Z]',' ',data[\"messages\"][0]).lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12c089b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go',\n",
       " 'until',\n",
       " 'jurong',\n",
       " 'point',\n",
       " 'crazy',\n",
       " 'available',\n",
       " 'only',\n",
       " 'in',\n",
       " 'bugis',\n",
       " 'n',\n",
       " 'great',\n",
       " 'world',\n",
       " 'la',\n",
       " 'e',\n",
       " 'buffet',\n",
       " 'cine',\n",
       " 'there',\n",
       " 'got',\n",
       " 'amore',\n",
       " 'wat']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the list containig the respective words\n",
    "\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b02524b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go',\n",
       " 'jurong',\n",
       " 'point',\n",
       " 'crazy',\n",
       " 'available',\n",
       " 'bugis',\n",
       " 'n',\n",
       " 'great',\n",
       " 'world',\n",
       " 'la',\n",
       " 'e',\n",
       " 'buffet',\n",
       " 'cine',\n",
       " 'got',\n",
       " 'amore',\n",
       " 'wat']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displayig the list of words afterting removing the stopwords\n",
    "\n",
    "[word for word in words if word not in stopwords.words(\"english\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1cd98270",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating object of PorterStemmer for performing suffix stripping which is a stemming approach\n",
    "\n",
    "ps=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6acbaf1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go',\n",
       " 'jurong',\n",
       " 'point',\n",
       " 'crazi',\n",
       " 'avail',\n",
       " 'bugi',\n",
       " 'n',\n",
       " 'great',\n",
       " 'world',\n",
       " 'la',\n",
       " 'e',\n",
       " 'buffet',\n",
       " 'cine',\n",
       " 'got',\n",
       " 'amor',\n",
       " 'wat']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps=PorterStemmer()\n",
    "\n",
    "#removing the stopwords followed by stemming the the suffix of the resultant list of words\n",
    "[ps.stem(word) for word in words if word not in stopwords.words(\"english\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb48d48e",
   "metadata": {},
   "source": [
    "# In above we can clearly observe that available, crazy, bugis and amore get stemmed to avail, crazi, bugi abd amor respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc857b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go jurong point crazi avail bugi n great world la e buffet cine got amor wat'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using join operation with space separation and generating the final sentence or document\n",
    "\n",
    "\" \".join([ps.stem(word) for word in words if word not in stopwords.words(\"english\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49473e50",
   "metadata": {},
   "source": [
    "'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e1e6d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "string=\"hello, world, hello\"\n",
    "pattern=r\"hello\"   # r represents read. So, r\"hello\" means read \"hello\"\n",
    "\n",
    "result=re.search(pattern,string) #using re.search to serach for the occurence of the pattern in string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e3b9ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 5), match='hello'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result\n",
    "#below output signifies that re.Match object in the span of 0th to 5th index found the matching\n",
    "#pattern 'hello' for the passed string.\n",
    "\n",
    "#Please note that re.search will only return the first occurence of the matching string. From below\n",
    "#output it is clear that it has not captured the span of second 'hello' substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20914f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pattern found\n"
     ]
    }
   ],
   "source": [
    "#Since matching pattern is found so result will be internally set to True\n",
    "\n",
    "if result:\n",
    "    print(\"pattern found\")\n",
    "else:\n",
    "    print(\"pattern not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1bfb509",
   "metadata": {},
   "outputs": [],
   "source": [
    "string=\"hello, world\"\n",
    "pattern=r\"sunny\"\n",
    "\n",
    "result=re.search(pattern,string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36671a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79b62023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pattern not found\n"
     ]
    }
   ],
   "source": [
    "#In this example since matching pattern is not found so result will be internally set to False and\n",
    "#hence we are getting output as patten not found\n",
    "\n",
    "if result:\n",
    "    print(\"pattern found\")\n",
    "else:\n",
    "    print(\"pattern not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "17fade2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello,python'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string=\"hello,world\"\n",
    "pattern=r\"world\"\n",
    "replacement=\"python\"\n",
    "\n",
    "re.sub(pattern,replacement,string)\n",
    "#Since we have passed the patten in read format so it will search the repective read pattern and will\n",
    "#perform substitution with the string passed with in replacement parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2885bbb",
   "metadata": {},
   "source": [
    "# Some use cases of regular expressions:\n",
    "\n",
    " 1. email validation\n",
    " 2. URL extraction\n",
    " 3. phone number extraction\n",
    " 4. password validation\n",
    " 5. HTML signup/login page field validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84a03dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarizing whaterver we have discussed so far\n",
    "\n",
    "corpus=[]\n",
    "for i in range(0,len(data)):\n",
    "    review=re.sub('[^a-zA-Z]',\" \",data[\"messages\"][i])\n",
    "    review=review.lower()\n",
    "    review=review.split()\n",
    "    review=[ps.stem(word) for word in review if word not in stopwords.words(\"english\")]\n",
    "    review=\" \".join(review)\n",
    "    corpus.append(review)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9d781425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pls go ahead with watts. I just wanted to be sure. Do have a great weekend. Abiola'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['messages'][40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5629be0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pl go ahead watt want sure great weekend abiola'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "17df9b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CountVectorizer uses Bag of Word Text to vector conversion approach\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165bc582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In BOW(Bag of Word) features are equivalent to vocabulary\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#By using max_features we are basically generating data set based on top vocabulary as defined using\n",
    "#max_feature. So through this we can overcome overfitting and do optimization. In the current example\n",
    "#we are setting max_features to 2500 which is equivalent to the top 2500 vocabularies(unique words).\n",
    "\n",
    "cv=CountVectorizer(max_features=2500)\n",
    "\n",
    "X=cv.fit_transform(corpus).toarray()  #Independent feature\n",
    "#We are transforming the obtained corpus using CountVectorizer with 2500 features followed by\n",
    "#transforming it to the array. Array transformation is done since it is fast as compared to pandas\n",
    "#dataframe.\n",
    "\n",
    "y=pd.get_dummies(data['labels'],drop_first=True)  #Dependent feature\n",
    "#get_dummies do class wise separate split. Read below markdown to know more about the get_dummies\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72580f0",
   "metadata": {},
   "source": [
    " \n",
    "# Read this beautiful article to understand more about .get_dummies:\n",
    "https://www.sharpsightlabs.com/blog/pandas-get-dummies/#:~:text=Report%20Ad-,drop_first,of%20the%20input%20categorical%20variable\n",
    " \n",
    " - A dummy variable is a numeric variable that encodes categorical information.\n",
    "\n",
    " - Dummy variables have two possible values: 0 or 1.\n",
    "\n",
    "- In a dummy variable:\n",
    "     -  1 encodes the presence of a category\n",
    "     - 0 encodes the absence of a category\n",
    "\n",
    " - drop_first = True ; So, it’s a common convention to drop the dummy variable for the first level of the categorical variable that you’re encoding.\n",
    "\n",
    " - (In fact, it’s frequently needed for some types of machine learning models. If you fail to drop the extra dummy variable, it can cause issues with your model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c096d3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a0de6032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8c57774c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2500)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d7c81037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cd0d92cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        ham\n",
       "1        ham\n",
       "2       spam\n",
       "3        ham\n",
       "4        ham\n",
       "        ... \n",
       "5567    spam\n",
       "5568     ham\n",
       "5569     ham\n",
       "5570     ham\n",
       "5571     ham\n",
       "Name: labels, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "15e74bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=pd.get_dummies(data['labels'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1f8ac043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      spam\n",
       "0        0\n",
       "1        0\n",
       "2        1\n",
       "3        0\n",
       "4        0\n",
       "...    ...\n",
       "5567     1\n",
       "5568     0\n",
       "5569     0\n",
       "5570     0\n",
       "5571     0\n",
       "\n",
       "[5572 rows x 1 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c2bc1eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "57d7e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset into Test and Train datasets\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7191900b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sunny\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9899497487437185\n",
      "0.9770279971284996\n"
     ]
    }
   ],
   "source": [
    "#We will be using multinomial naive bayes algorithm here since there are multiple categories present.\n",
    "#Also, since this is the text based classification ie; I/P is in the terms of text and not in terms of\n",
    "#categorical or numerical so, in all such intances we would prefer using Naive Bayes based approach\n",
    "#for building a Ml based learning model\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "model=MultinomialNB()\n",
    "model.fit(X_train,y_train)\n",
    "print(model.score(X_train,y_train))  #training accuracy score\n",
    "y_pred=model.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred)) #Testing accuracy score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "01b17234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"messages\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2e10e103",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we are taking a new data on which we would need to perform classification(Spam=1/Ham=0)\n",
    "\n",
    "data=\"Thanks gaurav for the update. This provides clear visibility for the next release.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a81b7e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thanks gaurav for the update. This provides clear visibility for the next release.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a12264d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got scalar array instead:\narray=Thanks gaurav for the update. This provides clear visibility for the next release..\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24100/3037987753.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     80\u001b[0m         \"\"\"\n\u001b[0;32m     81\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m         \u001b[0mjll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m_check_X\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_X\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[1;34m\"\"\"Validate X, used only in predict* methods.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    759\u001b[0m             \u001b[1;31m# If input is scalar raise error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 761\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m    762\u001b[0m                     \u001b[1;34m\"Expected 2D array, got scalar array instead:\\narray={}.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got scalar array instead:\narray=Thanks gaurav for the update. This provides clear visibility for the next release..\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "#Please note that here we are getting an error because data is passed directly to the model(to carry \n",
    "#out prediction) without performing any preprocessing(remove unwanted character using re, stopwords \n",
    "#removal, lower case conversion, stemming, appending to corpus, coverting corpus to array etc)\n",
    "\n",
    "model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "56179e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying all the above mentioned preprocessing steps over new data on which prediction needs to be\n",
    "#performed using the multinomial naive bayes ML leaning model\n",
    "\n",
    "review=re.sub('[^a-zA-Z]',\" \",data)\n",
    "review=review.lower()\n",
    "review=review.split()\n",
    "review=[ps.stem(word) for word in review if word not in stopwords.words(\"english\")]\n",
    "review=\" \".join(review)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ffa6ce5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "298f57e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank gaurav updat provid clear visibl next releas']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7f1a2136",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdata=cv.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6b589c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(tfdata)[0]\n",
    "\n",
    "#model predicted the new processed data as 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bce28b5",
   "metadata": {},
   "source": [
    "# For self practise we may use below on top of provided dataset. This consits of another approach of performing text based classification\n",
    "\n",
    "1. word_tokenizer = Apply this instead of .split() for tokenizing or coverting sentences into words.\n",
    "2. word lemetizer = Apply lemmitization instead of stemming that is being used in the current approach.\n",
    "3. tfidf = Apply tfidf instead of curently used CountVectorizer (bag of word) for converting text into vectors.\n",
    "4. gauusian naive bayes = Apply this instead of currently used multinomial naive bayes for building the text based classification learning ML model.\n",
    "\n",
    "https://github.com/sunnysavita10/Naive-Bayes/tree/main/Natural-Language-Processing\n",
    "\n",
    "Above GitHub repo can be useful in implementing the same\n",
    "\n",
    "Above approach may give an improved results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848c20fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
